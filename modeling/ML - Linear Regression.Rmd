---
title: "Linear Regression"
output: html_notebook
---

# 1. Load in packages
```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet) # for glmnet linear regression
library(Matrix) # for glmnet linear regression
library(kernlab) # for svm linear regression 
library(elasticnet) # for lasso linear regression 
```

# 2. Load in dataset we processed thus far in R
```{r}
student_por <- read.csv("../data/student-por_v3.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(student_por)
```

## Review the structure of loaded dataset
```{r}
str(student_por)
```

## Convert selected columns into factors
```{r}
student_por$famsize <- factor(student_por$famsize, ordered = TRUE, levels = c("LE3", "GT3"))
student_por$Medu <- factor(student_por$Medu, ordered = TRUE, levels = 0:4)
student_por$Fedu <- factor(student_por$Fedu, ordered = TRUE, levels = 0:4)
student_por$traveltime <- factor(student_por$traveltime, ordered = TRUE, levels = 1:4)
student_por$studytime <- factor(student_por$studytime, ordered = TRUE, levels = 1:4)
student_por$failures <- factor(student_por$failures, ordered = TRUE, levels = 0:4)
student_por$famrel <- factor(student_por$famrel, ordered = TRUE, levels = 1:5)
student_por$freetime <- factor(student_por$freetime, ordered = TRUE, levels = 1:5)
student_por$goout <- factor(student_por$goout, ordered = TRUE, levels = 1:5)
student_por$Dalc <- factor(student_por$Dalc, ordered = TRUE, levels = 1:5)
student_por$Walc <- factor(student_por$Walc, ordered = TRUE, levels = 1:5)
student_por$health <- factor(student_por$health, ordered = TRUE, levels = 1:5)
```

## Drop potential duplicate values for linear regression analysis 
```{r}
student_por$result <- NULL
```

## Check on the summary of loaded dataset again
```{r}
summary(student_por)
```
```{r}
str(student_por)
```

# 3. Prepare the loaded dataset for Linear Regression 
```{r}
# smoothed density estimates --> smoothed version of histogram for continuous data that comes from an underlying smooth distribution 
# plot the G3 / Final Grade target variable to check its' original skewness
ggplot(student_por) + geom_density(aes(x = G3)) + labs(x="Final Grade") + scale_x_continuous(limits = c(0, 20))
```
### It seems G3 or final grade is slightly skewed to the right, scaling and centralized will be performed.

# 4. Train - Test Data Split 
```{r}
set.seed(79)

student_por_idx <- createDataPartition(student_por$G3, p = 0.80, list = FALSE, times = 1)
student_por_trn <- student_por[student_por_idx, ]
student_por_tst <- student_por[-student_por_idx, ]
```

```{r}
nrow(student_por_trn)
```

```{r}
nrow(student_por_tst)
```

# 5. Modelling
## Define repeated 10 folds cross validation
```{r}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)
```

```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
  #getTrainPerf(caret_fit)
}
```

# Linear Regression 
### - method = ‘glmnet’
### - Type: Regression, Classification
### - Tuning parameters:
###     alpha (Mixing Percentage)
###     lambda (Regularization Parameter)
### - Required packages: glmnet, Matrix
### A model-specific variable importance metric is available.
```{r}
model.glmnet <- train(G3 ~. , 
               data = student_por_trn, 
               method = "glmnet", 
               trControl = fit_control) 
```

```{r}
model.glmnet
```

After tuning with tuneLength = 4, and defined hyperparameter variables. 
```{r}
hyperparams <- expand.grid(lambda = c(seq(0.1, 2.5, .05)), alpha = 1)

model.glmnet<- train(G3 ~. , 
               data = student_por_trn, 
               method = "glmnet", 
               trControl = fit_control, 
               preProc = c("center", "scale"),
               tuneGrid = hyperparams, 
               tuneLength = 4) 
```

```{r}
model.glmnet
```

```{r}
best.result <- round(get_best_result(model.glmnet),3)
best.result
```

```{r}
plot(model.glmnet)
```
#### Tuning done. 

# Predict G3 from test data 
```{r}
pred.result <- predict(model.glmnet, student_por_tst)
```

# Evaluate the result 
```{r}
glmnet.pred.result <- postResample(pred = pred.result, obs = student_por_tst$G3)
glmnet.pred.result
```

# Support Vector Machines with Linear Kernel
### method = 'svmLinear'
### Type: Regression, Classification
### Tuning parameters:
    C (Cost)
### Required packages: kernlab
```{r}
model.svm <- train(G3 ~. , 
               data = student_por_trn, 
               method = "svmLinear", 
               trControl = fit_control, 
               preProc = c("center", "scale")) 
```

```{r}
model.svm
```
```{r}
model.svm <- train(G3 ~. , 
               data = student_por_trn, 
               method = "svmLinear", 
               trControl = fit_control, 
               preProc = c("center", "scale"),
               tuneGrid = data.frame(.C = c(seq(0.25, 1.0, .25))),
               tuneLength = 4) 
```

```{r}
model.svm
```

```{r}
best.result.svm <- round(get_best_result(model.svm),3)
best.result.svm
```

```{r}
plot(model.svm)
```
#### Tuning done. 

# Predict G3 from test data 
```{r}
pred.result.svm <- predict(model.svm, student_por_tst)
```

# Evaluate the result 
```{r}
svm.pred.result <- postResample(pred = pred.result.svm, obs = student_por_tst$G3)
svm.pred.result
```

# The lasso
### method = 'lasso'
### Type: Regression
### Tuning parameters:
    Fraction (Fraction of Full Solution)
### Required packages: elasticnet
```{r}
model.lasso <- train(G3 ~. , 
               data = student_por_trn, 
               method = "lasso", 
               trControl = fit_control, 
               preProc = c("center", "scale"))
```

```{r}
model.lasso
```

```{r}
model.lasso <- train(G3 ~. , 
               data = student_por_trn, 
               method = "lasso", 
               trControl = fit_control, 
               preProc = c("center", "scale"),
               tuneLength = 4)
```


```{r}
best.result.lasso <- round(get_best_result(model.lasso),2)
best.result.lasso
```

```{r}
plot(model.lasso)
```
#### Tuning done. 

# Predict G3 from test data 
```{r}
pred.result.lasso <- predict(model.lasso, student_por_tst)
```

# Evaluate the result 
```{r}
lasso.pred.result <- postResample(pred = pred.result.lasso, obs = student_por_tst$G3)
lasso.pred.result
```

Comparison of performance between 3 models 
```{r}
print('glmnet linear regression model')
round(glmnet.pred.result,3)

print('svm linear regression model')
round(svm.pred.result,3) 

print('lasso linear regression model')
round(lasso.pred.result,3)
```